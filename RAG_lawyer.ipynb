{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgKmJ7C7pRam"
   },
   "source": [
    "# Mastering RAG Chatbots: From Implementation to Deployment\n",
    "\n",
    "## Course Objectives\n",
    "\n",
    "In this comprehensive tutorial, you'll learn how to:\n",
    "1. Understand Retrieval-Augmented Generation (RAG) architecture\n",
    "2. Implement a RAG-based chatbot from scratch\n",
    "3. Create a user-friendly web interface using Gradio\n",
    "4. Deploy and share your AI-powered document chat application\n",
    "\n",
    "### Prerequisites\n",
    "- Python programming skills\n",
    "- Basic understanding of machine learning\n",
    "- OpenAI API key\n",
    "- Google Colab environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k24yAqGhpRar"
   },
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Let's install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "# model_name = \"deepseek-ai/deepseek-llm-7b-chat\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16) #device_map=\"auto\")\n",
    "# model.generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "# model.generation_config.pad_token_id = model.generation_config.eos_token_id\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
    "# ]\n",
    "# input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "# outputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)\n",
    "\n",
    "# result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12468,
     "status": "ok",
     "timestamp": 1739289460835,
     "user": {
      "displayName": "Habiboulaye AMADOU B.",
      "userId": "00224232757473049450"
     },
     "user_tz": -60
    },
    "id": "-35DM6qbpRas",
    "outputId": "7fcd9740-920a-4072-f01a-95ea22227b4b"
   },
   "outputs": [],
   "source": [
    "# !pip install requests beautifulsoup4 PyPDF2 numpy gradio -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9853,
     "status": "ok",
     "timestamp": 1739289435659,
     "user": {
      "displayName": "Habiboulaye AMADOU B.",
      "userId": "00224232757473049450"
     },
     "user_tz": -60
    },
    "id": "bfSQ3sfYrRxN",
    "outputId": "8aea0990-4e6a-441b-9ede-516a6b08bdc9"
   },
   "outputs": [],
   "source": [
    "#!openai migrate\n",
    "# !pip install openai==0.28 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "NjIdupyGpRau"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "import PyPDF2\n",
    "#import openai\n",
    "import gradio as gr\n",
    "import ollama\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set OpenAI API Key\n",
    "#openai.api_key = \"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\"  # Replace with your actual key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mslgM3lQpRav"
   },
   "source": [
    "## 2. Content Extraction Modules\n",
    "\n",
    "We'll create functions to extract content from different sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DeepSeek API Key\n",
    "deepseek_client =OpenAI(api_key=\"sk-005d9d3de8d946ecbf9721293c44d6be\", base_url=\"https://api.deepseek.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "8yvGnFrGpRav"
   },
   "outputs": [],
   "source": [
    "# Extraction d'information depuis un site web\n",
    "# def scrape_website(url):\n",
    "#     \"\"\"Scrape text content from a website\"\"\"\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         return ' '.join(soup.stripped_strings)\n",
    "#     except Exception as e:\n",
    "#         return f\"Error scraping website: {str(e)}\"\n",
    "\n",
    "# Extraction d'information depuis un pdf\n",
    "def extract_pdf_content(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            return ' '.join(page.extract_text() for page in reader.pages)\n",
    "    except Exception as e:\n",
    "        return f\"Error processing PDF: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXHCVTuMpRaw"
   },
   "source": [
    "## 3. Text Processing and Embedding\n",
    "\n",
    "Implement text chunking and embedding generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "oZNXEGbvpRax"
   },
   "outputs": [],
   "source": [
    "def split_into_chunks(text, chunk_size=500):\n",
    "    \"\"\"Split text into manageable chunks\"\"\"\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "# def generate_embeddings(text):\n",
    "#     \"\"\"Generate embeddings using OpenAI\"\"\"\n",
    "#     response = openai.Embedding.create(\n",
    "#         model=\"text-embedding-ada-002\",\n",
    "#         input=text\n",
    "#     )\n",
    "#     return response['data'][0]['embedding']\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    \"\"\"Generate embeddings using Ollama\"\"\"\n",
    "    response = ollama.embeddings(model=\"llama3.2:1b\", prompt=text)\n",
    "    return response['embedding']\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fH8_Ax2xpRax"
   },
   "source": [
    "## 4. RAG Chatbot Core Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "5BP5sdEIpRay"
   },
   "outputs": [],
   "source": [
    "class RAGChatbot:\n",
    "    def __init__(self):\n",
    "        self.chunks_with_embeddings = None\n",
    "\n",
    "    def load_from_url(self, url):\n",
    "        \"\"\"Load content from a website\"\"\"\n",
    "        content = scrape_website(url)\n",
    "        self._process_content(content)\n",
    "\n",
    "    def load_from_pdf(self, pdf_path):\n",
    "        \"\"\"Load content from a PDF\"\"\"\n",
    "        content = extract_pdf_content(pdf_path)\n",
    "        self._process_content(content)\n",
    "\n",
    "    def _process_content(self, content):\n",
    "        \"\"\"Process content into chunks and generate embeddings\"\"\"\n",
    "        chunks = split_into_chunks(content)\n",
    "        self.chunks_with_embeddings = [\n",
    "            {\"content\": chunk, \"embedding\": generate_embeddings(chunk)}\n",
    "            for chunk in chunks\n",
    "        ]\n",
    "\n",
    "    def find_relevant_chunk(self, query):\n",
    "        \"\"\"Find most relevant text chunk for a query\"\"\"\n",
    "        query_embedding = generate_embeddings(query)\n",
    "        similarities = [\n",
    "            (chunk[\"content\"], cosine_similarity(query_embedding, chunk[\"embedding\"]))\n",
    "            for chunk in self.chunks_with_embeddings\n",
    "        ]\n",
    "        return max(similarities, key=lambda x: x[1])[0]\n",
    "\n",
    "    # def ask(self, query):\n",
    "    #     \"\"\"Generate response based on query and context\"\"\"\n",
    "    #     if not self.chunks_with_embeddings:\n",
    "    #         return \"Please load content first using load_from_url or load_from_pdf\"\n",
    "\n",
    "        # relevant_chunk = self.find_relevant_chunk(query)\n",
    "        # response = openai.ChatCompletion.create(\n",
    "        #     #model=\"gpt-3.5-turbo\",\n",
    "        #     model=\"gpt-4o-mini\",\n",
    "        #     messages=[\n",
    "        #         {\"role\": \"system\", \"content\": \"You are a helpful assistant using context to answer questions.\"},\n",
    "        #         {\"role\": \"user\", \"content\": f\"Context: {relevant_chunk}\\n\\nQuery: {query}\"}\n",
    "        #     ],\n",
    "        #     max_tokens=200\n",
    "        # )\n",
    "        # return response['choices'][0]['message']['content']\n",
    "\n",
    "    def ask(self, query):\n",
    "        \"\"\"Generate response based on query and context using Ollama\"\"\"\n",
    "        if not self.chunks_with_embeddings:\n",
    "            return \"Please load content first using load_from_url or load_from_pdf\"\n",
    "\n",
    "        relevant_chunk = self.find_relevant_chunk(query)\n",
    "        \n",
    "        response = ollama.chat(\n",
    "            # model=\"llama3.2:1b\",  # Tu peux changer pour \"llama3\" ou un autre modÃ¨le\n",
    "            model=\"deepseek-llm\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an GDPR expert using context to answer questions.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Context: {relevant_chunk}\\n\\nQuery: {query}\"}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return response['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOHn2-v-pRaz"
   },
   "source": [
    "## 5. Gradio Web Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "kUNYzV_JpRa0"
   },
   "outputs": [],
   "source": [
    "# --- ipython-input-7-7774a7eacb87 ---\n",
    "class RAGChatbotInterface:\n",
    "    def __init__(self):\n",
    "        self.chatbot = RAGChatbot()\n",
    "        self.chat_history = []\n",
    "\n",
    "    def process_file(self, file):\n",
    "        \"\"\"Process uploaded PDF file\"\"\"\n",
    "        try:\n",
    "            self.chatbot.load_from_pdf(file.name)\n",
    "            return \"PDF successfully loaded! You can now ask questions.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error processing PDF: {str(e)}\"\n",
    "\n",
    "    def process_url(self, url):\n",
    "        \"\"\"Process website URL\"\"\"\n",
    "        try:\n",
    "            self.chatbot.load_from_url(url)\n",
    "            return \"Website content successfully loaded! You can now ask questions.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error processing URL: {str(e)}\"\n",
    "\n",
    "    def chat(self, message, history):\n",
    "        \"\"\"Process chat message and update history\"\"\"\n",
    "        try:\n",
    "            response = self.chatbot.ask(message)\n",
    "            history.append((message, response))\n",
    "            return response, history\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error generating response: {str(e)}\"\n",
    "            history.append((message, error_message))\n",
    "            return error_message, history\n",
    "\n",
    "    def launch_interface(self, share=True):\n",
    "        \"\"\"Create and launch Gradio interface\"\"\"\n",
    "        with gr.Blocks(title=\"RAG Chatbot\") as interface:\n",
    "            gr.Markdown(\"# ðŸ“š RAG Chatbot: Learn from Any Document\")\n",
    "\n",
    "            with gr.Tab(\"PDF Input\"):\n",
    "                # Change 'type' to 'filepath' to get the file path\n",
    "                pdf_upload = gr.File(label=\"Upload PDF\", type=\"filepath\", file_types=[\".pdf\"])\n",
    "                pdf_status = gr.Textbox(label=\"PDF Status\", interactive=False)\n",
    "                pdf_upload.upload(fn=self.process_file, inputs=[pdf_upload], outputs=[pdf_status])\n",
    "\n",
    "            with gr.Tab(\"URL Input\"):\n",
    "                url_input = gr.Textbox(label=\"Enter Website URL\", placeholder=\"https://example.com\")\n",
    "                url_status = gr.Textbox(label=\"URL Status\", interactive=False)\n",
    "                url_button = gr.Button(\"Load Content\")\n",
    "                url_button.click(fn=self.process_url, inputs=[url_input], outputs=[url_status])\n",
    "\n",
    "            chatbot = gr.Chatbot(label=\"Chat with Your Document\", height=400)\n",
    "            msg = gr.Textbox(label=\"Your Question\", placeholder=\"Ask a question about the document...\")\n",
    "            clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "            msg.submit(fn=self.chat, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
    "            clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "        interface.launch(share=share)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9NvffjppRa0"
   },
   "source": [
    "## 6. Launching the RAG Chatbot Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1739289533383,
     "user": {
      "displayName": "Habiboulaye AMADOU B.",
      "userId": "00224232757473049450"
     },
     "user_tz": -60
    },
    "id": "eG3T2eoOpRa1",
    "outputId": "e0b81f0a-d2c8-4517-bc9b-912e1498352a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdou\\AppData\\Local\\Temp\\ipykernel_7220\\4037565904.py:51: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Chat with Your Document\", height=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and launch the interface\n",
    "rag_interface = RAGChatbotInterface()\n",
    "rag_interface.launch_interface(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XccK1E4ApRa1"
   },
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "By completing this tutorial, you will have learned:\n",
    "\n",
    "1. **Content Extraction Techniques**\n",
    "   - Web scraping with BeautifulSoup\n",
    "   - PDF text extraction\n",
    "\n",
    "2. **Text Processing**\n",
    "   - Text chunking strategies\n",
    "   - Semantic embedding generation\n",
    "\n",
    "3. **RAG Architecture**\n",
    "   - Context retrieval\n",
    "   - Similarity-based chunk selection\n",
    "   - Prompt engineering\n",
    "\n",
    "4. **Web Interface Development**\n",
    "   - Creating interactive UIs with Gradio\n",
    "   - Handling file and URL inputs\n",
    "   - Managing chat interactions\n",
    "\n",
    "## Challenges and Extensions\n",
    "\n",
    "1. Implement multi-document support\n",
    "2. Add more sophisticated embedding techniques\n",
    "3. Improve error handling and input validation\n",
    "4. Create a more advanced prompt engineering strategy\n",
    "5. Implement conversation memory\n",
    "\n",
    "## Ethical Considerations\n",
    "\n",
    "- Always respect copyright and terms of service\n",
    "- Be mindful of privacy when processing documents\n",
    "- Use AI responsibly and ethically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
