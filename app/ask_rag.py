# ask_rag.py

from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
import requests

# === CONFIGURATION ===
LLM_API_URL = "http://localhost:1234/v1/chat/completions"  # LM Studio API
COLLECTION_NAME = "rgpd_chunks"
QDRANT_HOST = "localhost"
QDRANT_PORT = 6333

# === INITIALISATION ===
print("ğŸš€ Chargement du modÃ¨le d'embedding...")
embedder = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

print("ğŸ§  Connexion Ã  Qdrant...")
qdrant = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

def ask_llm(context, question):
    """Envoie un prompt au LLM local via LM Studio."""
    prompt = f"""
Voici des extraits du RGPD :
{context}

En te basant uniquement sur ces extraits, rÃ©ponds prÃ©cisÃ©ment Ã  la question suivante :
{question}
"""

    response = requests.post(
        LLM_API_URL,
        headers={"Content-Type": "application/json"},
        json={
            "model": "local-model",  # IgnorÃ© par LM Studio, mais requis
            "messages": [
                {"role": "system", "content": "Tu es un assistant juridique expert du RGPD."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.2
        }
    )

    try:
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Erreur de LLM : {e}"

def retrieve_context(query, top_k=5):
    """Effectue une recherche sÃ©mantique dans Qdrant et retourne les textes associÃ©s."""
    query_vector = embedder.encode(query)
    results = qdrant.search(
        collection_name=COLLECTION_NAME,
        query_vector=query_vector,
        limit=top_k
    )
    return "\n---\n".join([r.payload["text"] for r in results])

# === INTERFACE UTILISATEUR ===
if __name__ == "__main__":
    print("ğŸ§‘â€ğŸ’¼ SystÃ¨me RAG RGPD prÃªt. Pose ta question (ou tape 'exit') ğŸ‘‡\n")

    while True:
        question = input("â“> ").strip()
        if question.lower() in ["exit", "quit"]:
            break

        print("ğŸ” Recherche des documents pertinents...")
        context = retrieve_context(question)

        print("\nğŸ¤– GÃ©nÃ©ration de la rÃ©ponse...\n")
        response = ask_llm(context, question)
        print("ğŸ’¬ RÃ©ponse :\n")
        print(response)
        print("\n" + "=" * 60 + "\n")
